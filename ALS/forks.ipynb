{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SQLContext\n",
    "sc = pyspark.SparkContext()\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionsData:\n",
    "    \n",
    "    def __init__(self,folder,file,file_repos,user,date,item):\n",
    "        self._data=None\n",
    "        self._data_items=None\n",
    "        self.load_data(folder,file,user,date,item)\n",
    "        self.load_items(folder,file_repos)\n",
    "            \n",
    "    def load_data(self,foldername,filename,user,date,item):\n",
    "        \"\"\"load interactions data of users with repositories\"\"\"\n",
    "        file=os.path.join(foldername+filename)\n",
    "        data=sqlc.read.json(file)\n",
    "        data=data.select(col(user).alias('user_id'),col(date).alias('created_at'), col(item).alias('repo_id'))\n",
    "        self._data=data\n",
    "        \n",
    "    def load_items(self,foldername,filename):\n",
    "        \"\"\"load informations about repositories\"\"\"\n",
    "        file_repos=os.path.join(foldername+filename)\n",
    "        data_repos=sqlc.read.json(file_repos)\n",
    "        self._data_items=data_repos.select(col('id').alias('repo_id'),'name','language').distinct()\n",
    "    \n",
    "    def join_w_repos(self):\n",
    "        \"\"\"consider only interactions to repositories contained in self._data_items\"\"\"\n",
    "        self._data=self._data.join(self._data_items,'repo_id','inner')\n",
    "    \n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"remove duplicated of interactions of a user with the same repository\"\"\"\n",
    "        self._data=self._data.sort('user_id','created_at',ascending=True).dropDuplicates(['user_id','repo_id'])\n",
    "\n",
    "    def filter_actions(self,min_actions,max_actions):\n",
    "        \"\"\"filter out users inactive users (users who interacted with less than min_actions\n",
    "        repositories) and outliers (users who interacted with more than max_actions repositories)\"\"\"\n",
    "        data_with_max=self._data.groupby('user_id').agg(F.count('repo_id').alias('total_actions'))\n",
    "        data_filter=data_with_max.filter((data_with_max.total_actions>min_actions)\\\n",
    "                                           & (data_with_max.total_actions<max_actions))\n",
    "        \n",
    "        self._data=self._data.join(data_filter.select('user_id'),'user_id','inner')\n",
    "\n",
    "        \n",
    "    def add_rating(self,rating):\n",
    "        \"\"\"add a column with rating value: in a class instance each interaction has the same value\"\"\"\n",
    "        self._data=self._data.groupby('user_id','created_at','repo_id')\\\n",
    "                            .agg((F.count('*')*rating).alias('rating'))\n",
    "        \n",
    "        \n",
    "    def transform(self,min_actions,max_actions,rating):\n",
    "        \"\"\"apply data transformations\"\"\"\n",
    "        self.join_w_repos()\n",
    "        self.remove_duplicates()\n",
    "        self.filter_actions(min_actions,max_actions)\n",
    "        self.add_rating(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleRecommender:\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        \n",
    "        self._data=data\n",
    "        self._train=None\n",
    "        self._test=None\n",
    "        self._model=None\n",
    "        self._predictions_train=None\n",
    "        self._predictions_test=None\n",
    "    \n",
    "    \n",
    "    def message(self,x):\n",
    "        print(x)\n",
    "        \n",
    "    def split_train_test(self):\n",
    "        self._train=self._data.filter('number_of_actions<total_actions')\n",
    "        self._test=self._data.filter('number_of_actions=total_actions')\n",
    "         \n",
    "    def fit(self,param):\n",
    "        self.split_train_test()\n",
    "        als = ALS(maxIter=param['iter'],rank=param['rank'],regParam=param['reg'],userCol=\"user_idn\",\\\n",
    "                    itemCol=\"repo_idn\",ratingCol=\"rating\", seed=1, coldStartStrategy='drop')\n",
    "        evaluator_reg=RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")      \n",
    "        model=als.fit(self._train)\n",
    "        self._model=model\n",
    "        self._predictions_train=model.transform(self._train)\n",
    "        train_rmse=evaluator_reg.evaluate(self._predictions_train)\n",
    "        self.message('Train RMSE=' + str(train_rmse))\n",
    "        self._predictions_test=model.transform(self._test)\n",
    "        test_rmse=evaluator_reg.evaluate(self._predictions_test)\n",
    "        self.message('Test RMSE=' + str(test_rmse))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load forks data: consider only users who forked between 5 and 2500 repositories. Each fork is given a value of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks=ActionsData(folder='./data',file='projects_forked_2017.json',\\\n",
    "                  file_repos='projects_not_forked_2017.json',\\\n",
    "                  user='owner_id',date='created_at',item='forked_from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks.transform(min_actions=5, max_actions=2500,rating=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks_data=forks._data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, created_at: string, repo_id: string, rating: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forks_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two additional columns: number_of_actions is the sequential number of forks by user ordered by date; total_actions is the total number of forks by user. These two columns will be used to split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=(Window.partitionBy('user_id').orderBy('created_at').rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "forks_data=forks_data.withColumn('number_of_actions',F.count('user_id').over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_actions=forks_data.groupby('user_id').agg(F.max('number_of_actions').alias('total_actions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks_data=forks_data.join(total_actions,'user_id','inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numeric IDs for users and repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer_user=StringIndexer(inputCol=\"user_id\",outputCol=\"user_idn\")#.setHandleInvalid('skip')\n",
    "indexer_repo=StringIndexer(inputCol='repo_id',outputCol='repo_idn')\n",
    "forks_data=indexer_user.fit(forks_data).transform(forks_data)\n",
    "forks_data=indexer_repo.fit(forks_data).transform(forks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, created_at: string, repo_id: string, rating: bigint, number_of_actions: bigint, total_actions: bigint, user_idn: double, repo_idn: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forks_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SimpleRecommender instance, fit the model and calculate the error. Save the model in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec=SimpleRecommender(forks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALS with maxIter=20, rank=20, regParameter=0\n",
      "Train RMSE=0.10063693100366188\n",
      "Test RMSE=0.19723813927147596\n"
     ]
    }
   ],
   "source": [
    "parameters={'rank':20,'iter':20,'reg':0.1}\n",
    "\n",
    "rec.fit(param=parameters)\n",
    "model_final=rec._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.save('/data/als_r20_i20_reg01_f.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
