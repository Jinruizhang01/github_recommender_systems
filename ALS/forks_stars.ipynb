{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "sc = pyspark.SparkContext()\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionsData:\n",
    "    \n",
    "    def __init__(self,folder,file,file_repos,user,date,item):\n",
    "        self._data=None\n",
    "        self._data_items=None\n",
    "        self.load_data(folder,file,user,date,item)\n",
    "        self.load_items(folder,file_repos)\n",
    "            \n",
    "    def load_data(self,foldername,filename,user,date,item):\n",
    "        \"\"\"load interactions data of users with repositories\"\"\"\n",
    "        file=os.path.join(foldername+filename)\n",
    "        data=sqlc.read.json(file)\n",
    "        data=data.select(col(user).alias('user_id'),col(date).alias('created_at'), col(item).alias('repo_id'))\n",
    "        self._data=data\n",
    "        \n",
    "    def load_items(self,foldername,filename):\n",
    "        \"\"\"load informations about repositories\"\"\"\n",
    "        file_repos=os.path.join(foldername+filename)\n",
    "        data_repos=sqlc.read.json(file_repos)\n",
    "        self._data_items=data_repos.select(col('id').alias('repo_id'),'name','language').distinct()\n",
    "    \n",
    "    def join_w_repos(self):\n",
    "        \"\"\"consider only interactions to repositories contained in self._data_items\"\"\"\n",
    "        self._data=self._data.join(self._data_items,'repo_id','inner')\n",
    "    \n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"remove duplicated of interactions of a user with the same repository\"\"\"\n",
    "        self._data=self._data.sort('user_id','created_at',ascending=True).dropDuplicates(['user_id','repo_id'])\n",
    "\n",
    "    def filter_actions(self,min_actions,max_actions):\n",
    "        \"\"\"filter out users inactive users (users who interacted with less than min_actions\n",
    "        repositories) and outliers (users who interacted with more than max_actions repositories)\"\"\"\n",
    "        data_with_max=self._data.groupby('user_id').agg(F.count('repo_id').alias('total_actions'))\n",
    "        data_filter=data_with_max.filter((data_with_max.total_actions>min_actions)\\\n",
    "                                           & (data_with_max.total_actions<max_actions))\n",
    "        \n",
    "        self._data=self._data.join(data_filter.select('user_id'),'user_id','inner')\n",
    "\n",
    "        \n",
    "    def add_rating(self,rating):\n",
    "        \"\"\"add a column with rating value: in a class instance each interaction has the same value\"\"\"\n",
    "        self._data=self._data.groupby('user_id','created_at','repo_id')\\\n",
    "                            .agg((F.count('*')*rating).alias('rating'))\n",
    "        \n",
    "        \n",
    "    def transform(self,min_actions,max_actions,rating):\n",
    "        \"\"\"apply data transformations\"\"\"\n",
    "        self.join_w_repos()\n",
    "        self.remove_duplicates()\n",
    "        self.filter_actions(min_actions,max_actions)\n",
    "        self.add_rating(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleRecommender:\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        \n",
    "        self._data=data\n",
    "        self._train=None\n",
    "        self._test=None\n",
    "        self._model=None\n",
    "    \n",
    "    \n",
    "    def message(self,x):\n",
    "        print(x)\n",
    "        \n",
    "    def split_train_test(self):\n",
    "        self._train=self._data.filter('number_of_actions<total_actions')\n",
    "        self._test=self._data.filter('number_of_actions=total_actions')\n",
    "         \n",
    "    def fit(self,param):\n",
    "        self.split_train_test()\n",
    "        self.message('Train ALS with maxIter=%d, rank=%d, regParameter=%d.d' % (param['iter'],param['rank'],param['reg']))\n",
    "        \n",
    "        als = ALS(maxIter=param['iter'],rank=param['rank'],regParam=param['reg'],userCol=\"user_idn\",\\\n",
    "                    itemCol=\"repo_idn\",ratingCol=\"rating\", seed=1, coldStartStrategy='drop')\n",
    "        evaluator_reg=RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")      \n",
    "        model=als.fit(self._train)\n",
    "        self._model=model\n",
    "        predictions_train=model.transform(self._train)\n",
    "        train_rmse=evaluator_reg.evaluate(predictions_train)\n",
    "        self.message('Train RMSE=' + str(train_rmse))\n",
    "        predictions_train=predictions_train.withColumn('prediction_bin',when(col('prediction')>0.5,1).otherwise(0))\n",
    "        multiclass_train= MulticlassMetrics(predictions_train.select(col('prediction_bin').\\\n",
    "                                          alias('score'),col('rating').alias('label')).\\\n",
    "                                          rdd.map(lambda x: (float(x.score),float(x.label))))\n",
    "        train_rec=multiclass_train.recall()\n",
    "        self.message('Train Recall=' + str(train_rec))\n",
    "        predictions_test=model.transform(self._test)\n",
    "        test_rmse=evaluator_reg.evaluate(predictions_test)\n",
    "        self.message('Test RMSE=' + str(test_rmse))\n",
    "        predictions_test=predictions_test.withColumn('prediction_bin',when(col('prediction')>0.5,1).otherwise(0))\n",
    "        multiclass_test= MulticlassMetrics(predictions_test.select(col('prediction_bin').\\\n",
    "                                          alias('score'),col('rating').alias('label')).\\\n",
    "                                          rdd.map(lambda x: (float(x.score),float(x.label))))\n",
    "        test_rec=multiclass_test.recall()\n",
    "        self.message('Test Recall=' + str(test_rec))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load forks data: consider only users who forked between 5 and 2500 repositories. Each fork has rating value=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks=ActionsData(folder='./data',file='projects_forked_2017.json',\\\n",
    "                  file_repos='projects_not_forked_2017.json',\\\n",
    "                  user='owner_id',date='created_at',item='forked_from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks.transform(min_actions=5, max_actions=2500,rating=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks_data=forks._data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, created_at: string, repo_id: string, rating: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forks_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+------+\n",
      "| user_id|          created_at| repo_id|rating|\n",
      "+--------+--------------------+--------+------+\n",
      "|10013904|2017-03-23 21:00:...|58639826|     1|\n",
      "|10013904|2017-06-20 05:14:...|66659683|     1|\n",
      "|10013904|2017-05-06 05:16:...|60058077|     1|\n",
      "|10013904|2017-03-27 20:22:...|57414261|     1|\n",
      "|10013904|2017-06-21 19:50:...|60254642|     1|\n",
      "+--------+--------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forks_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load forks data: consider only users who forked between 20 and 7000 repositories. Each fork has rating value=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars=ActionsData(folder='./data',file='watchers_2017.json',\\\n",
    "                  file_repos='projects_not_forked_2017.json',\\\n",
    "                  user='w_user_id',date='w_created_at',item='w_repo_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars.transform(min_actions=20, max_actions=7000,rating=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars_data=stars._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, created_at: string, repo_id: string, rating: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+------+\n",
      "|user_id|          created_at| repo_id|rating|\n",
      "+-------+--------------------+--------+------+\n",
      "| 101122|2017-04-27 02:43:...|63294245|     1|\n",
      "| 101122|2017-07-15 01:51:...|65917216|     1|\n",
      "| 101122|2017-08-08 07:32:...|70845318|     1|\n",
      "| 101122|2017-03-26 06:21:...|56265831|     1|\n",
      "| 101122|2017-08-31 04:23:...|73197055|     1|\n",
      "+-------+--------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stars_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the union data of forks and stars and drop duplicates (users that forked and starred the same repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, created_at: string, repo_id: string, rating: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union=forks_data.union(stars_data)\n",
    "union_dd=union.sort('user_id','created_at').dropDuplicates(['user_id','repo_id'])\n",
    "union_dd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two additional columns: number_of_actions is the sequential number of interations (forks,stars) by user ordered by date; total_actions is the total number of interactions by user. These two columns will be used to split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=(Window.partitionBy('user_id').orderBy('created_at').rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "union_dd=union_dd.withColumn('number_of_actions',F.count('user_id').over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_actions=union_dd.groupby('user_id').agg(F.max('number_of_actions').alias('total_actions'))\n",
    "union_dd=union_dd.join(total_actions,'user_id','inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numeric IDs for users and repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer_user=StringIndexer(inputCol=\"user_id\",outputCol=\"user_idn\")#.setHandleInvalid('skip')\n",
    "indexer_repo=StringIndexer(inputCol='repo_id',outputCol='repo_idn')\n",
    "union_dd=indexer_user.fit(union_dd).transform(union_dd)\n",
    "union_dd=indexer_repo.fit(union_dd).transform(union_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a recommendation model with SimpleRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec=SimpleRecommender(union_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ALS with maxIter=10, rank=10, regParameter=0.d\n",
      "Train RMSE=0.09996531920783683\n",
      "Train Recall=0.9999653022641679\n",
      "Test RMSE=0.14636304894348268\n",
      "Test Recall=0.9875230485556239\n"
     ]
    }
   ],
   "source": [
    "parameters={'rank':10,'iter':10,'reg':0.1}\n",
    "rec.fit(param=parameters)\n",
    "\n",
    "model_final=rec._model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.save('/data/als_r10_i10_reg01_fs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
